{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Cuda Vision Lab project soccerperception.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GjgBaN0olS0"
      },
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import randperm\n",
        "from torch._utils import _accumulate\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numbers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import randperm\n",
        "from torch._utils import _accumulate\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision.transforms.functional as F\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from skimage import io, transform\n",
        "import sklearn.metrics as skm\n",
        "import imutils\n",
        "from scipy.spatial import distance\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK3DsdQx2EsM"
      },
      "source": [
        "Find bounding box given 2 points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TRclibEVDkt"
      },
      "source": [
        "def find_bound_box(values,label,factor):\n",
        "   xmin = int(int((values.find('xmin').text))*factor/4)\n",
        "   ymin = int(int((values.find('ymin').text))*factor/4)\n",
        "   xmax = int(int((values.find('xmax').text))*factor/4)\n",
        "   ymax = int(int((values.find('ymax').text))*factor/4)\n",
        "   if label == 0:\n",
        "     centery = int((ymin+ymax)/2)\n",
        "   else:\n",
        "     centery = ymin\n",
        "   return np.array((xmin,ymin,xmax,ymax,int((xmin+xmax)/2),centery,np.abs(xmax-xmin),np.abs(ymin-ymax))).reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skckCiXfqCJ7"
      },
      "source": [
        "Generating the heatmaps given the landmarks of the objects\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "648rkgV4VQzE"
      },
      "source": [
        "def generate_heatMap(label,img_heatmap,bnddtls):\n",
        "    objects_colors = [[255,255,0],[255,0,255],[0,255,255]]\n",
        "    radius = [9 ,3 , 15]\n",
        "    #cyan for ball: 0,255,255 - magenta for goalposts: 255,0,255 - yellow for robot: 255,255,0\n",
        "    size=radius[label] #aperture\n",
        "    rad = radius[label]\n",
        "    x, y = np.meshgrid(np.linspace(-1, 1, rad), np.linspace(-1, 1, rad))\n",
        "    d = np.sqrt(x * x + y * y)\n",
        "    sigma, mu = 1.0, 0.0\n",
        "    kernel = np.exp(-((d - mu) ** 2 / (2.0 * sigma ** 2)))\n",
        "\n",
        "    uniq = np.unique(kernel)\n",
        "    halfsize = int(uniq.shape[0] /2)\n",
        "    mean = uniq[halfsize]\n",
        "\n",
        "    size = int(size/2)\n",
        "\n",
        "    if bnddtls[2][1].item()+size > img_heatmap.shape[0]-1:\n",
        "                y_begin = img_heatmap.shape[0]-1-size\n",
        "    elif bnddtls[2][1].item()-size < 0:\n",
        "                y_begin = bnddtls[2][1] + size\n",
        "    else:\n",
        "                y_begin = int(bnddtls[2][1].item())\n",
        "\n",
        "    if bnddtls[2][0].item()+size > img_heatmap.shape[1]-1:\n",
        "                x_begin = img_heatmap.shape[1]-1-size\n",
        "    elif bnddtls[2][0].item()-size < 0:\n",
        "                x_begin = bnddtls[2][0] + size\n",
        "\n",
        "    else:\n",
        "                x_begin = int(bnddtls[2][0].item())\n",
        "\n",
        "    heatmap_value = img_heatmap[(y_begin -size) : (y_begin + (size+1)), (x_begin - size) : (x_begin + (size+1))]\n",
        "\n",
        "    heatmap_value[kernel >= mean] = objects_colors[label]\n",
        "    \n",
        "    img_heatmap[(y_begin -size) : (y_begin + (size+1)), (x_begin - size) : (x_begin + (size+1))] = heatmap_value\n",
        "     \n",
        "    return img_heatmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrXcyZP2qMxE"
      },
      "source": [
        "Creating the heatmaps: 1- reading all the images and lanmarks of the detection dataset. \n",
        "2- calling the generating_heatmap function to generate heatmap for each image.\n",
        "3- Save the generated heatmap in the dataset path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7f8H7qpV_zk"
      },
      "source": [
        "def create_heatMap_images(root_dir):\n",
        "  labels = {\"ball\":0 , \"goalpost\":1 , \"robot\":2}\n",
        "\n",
        "  k = 0\n",
        "  for file in os.listdir(root_dir):\n",
        "    if file.endswith(\".jpg\"):\n",
        "      extension = \".jpg\"\n",
        "    elif file.endswith(\".jpeg\"):\n",
        "      extension = \".jpeg\"\n",
        "    elif file.endswith(\".png\") and not file.endswith(\"_heatmap.png\"):\n",
        "      extension = \".png\"\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "    filepath = os.path.join(root_dir,file)\n",
        "\n",
        "    xml_file = filepath.replace(extension,'.xml')\n",
        "    filename = filepath.replace(extension,'_heatmap.png')\n",
        "\n",
        "    if os.path.isfile(xml_file):\n",
        "      if (file.startswith('b')):\n",
        "        factor = 0.8\n",
        "      else:\n",
        "        factor =1\n",
        "      image = cv2.imread(filepath,cv2.COLOR_BGR2RGB)\n",
        "      new_shape = (int(image.shape[0]/4),int(image.shape[1]/4),3)\n",
        "      img_heatmap = np.ones(new_shape) * 255\n",
        "      xml_data = ET.parse(xml_file).getroot()\n",
        "\n",
        "      all_dtls= []\n",
        "\n",
        "      all_objects_list = xml_data.findall('object')\n",
        "\n",
        "      for group in all_objects_list:\n",
        "          bndvalues = group.find('bndbox')\n",
        "          name = group.find('name').text\n",
        "          label = labels[name]\n",
        "          land_marks = find_bound_box(bndvalues,label,factor)\n",
        "          img_heatmap = generate_heatMap(label,img_heatmap,land_marks)\n",
        "          \n",
        "      cv2.imwrite(filename, img_heatmap)\n",
        "      k+=1\n",
        "      \n",
        "  print(\"done \", k)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFGUcTqrrAPL"
      },
      "source": [
        "This function is run before the training to save all the heatmaps in the dataset path, to save the time of generating the heatmaps during training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cGzciO5q_ue"
      },
      "source": [
        "#create_heatMap_images('/content/drive/My Drive/detection/dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVARSgmc0nEt"
      },
      "source": [
        "Defining the global variables that are needed in the Custom Dataset Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGPQM6YmAhdd"
      },
      "source": [
        "resize_mode = \"resize\"\n",
        "resize_factor = 1\n",
        "mode = \"\"\n",
        "trs = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "                 transforms.ToTensor()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glSIEGAK5rHz"
      },
      "source": [
        "Custom Dataset for  Detection Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jewfctGqpK04"
      },
      "source": [
        "class DetectionDataset(Dataset):\n",
        "    \"\"\"Cutomized Dataset used to train the model.\n",
        "    Args:\n",
        "      root_dir: path where all the training files are saved.\n",
        "      transform: transformations to be applied to the dataset.\n",
        "      filenames: names of all the files in training dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, transformm=None):\n",
        "        super(DetectionDataset, self).__init__()\n",
        "        self.root_dir = root_dir\n",
        "        global trs\n",
        "        self.filenames = []\n",
        "        self.heatmap_filenames = []\n",
        "        self.xml_filenames = []\n",
        "        global resize_mode\n",
        "        global resize_factor\n",
        "        global mode \n",
        "\n",
        "        for file in os.listdir(root_dir):\n",
        "            filepath = os.path.join(root_dir,file)\n",
        "            \n",
        "            if file.endswith(\".jpg\"):\n",
        "              extension = \".jpg\"\n",
        "            elif file.endswith(\".jpeg\"):\n",
        "              extension = \".jpeg\"\n",
        "            elif file.endswith(\".png\") and not file.endswith(\"_heatmap.png\"):\n",
        "              extension = \".png\"\n",
        "            else:\n",
        "              continue\n",
        "\n",
        "            heatmap_file_path = filepath.replace(extension,\"_heatmap.png\")\n",
        "\n",
        "            if os.path.isfile(heatmap_file_path):\n",
        "              xml_file = file.replace(extension,'.xml')\n",
        "              heatmapfile = file.replace(extension,\"_heatmap.png\")\n",
        "              self.heatmap_filenames.append(heatmapfile)\n",
        "              self.filenames.append(file)\n",
        "              self.xml_filenames.append(xml_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Length of the dataset\"\"\"\n",
        "        return len(self.heatmap_filenames)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        filepath = os.path.join(self.root_dir,self.filenames[ind])\n",
        "        heatmap_path = os.path.join(self.root_dir,self.heatmap_filenames[ind])\n",
        "\n",
        "        img = Image.open(filepath)\n",
        "        heatmap = Image.open(heatmap_path)\n",
        "        inputImage = img.copy()\n",
        "\n",
        "        if(resize_mode==\"resize\"):\n",
        "          img = img.resize((int(64*resize_factor),int(32*resize_factor)),Image.NEAREST)\n",
        "          heatmap = heatmap.resize((int(16*resize_factor),int(8*resize_factor)),Image.NEAREST)\n",
        "\n",
        "        if trs:\n",
        "            if type(trs) is not list:\n",
        "              transform = [trs]\n",
        "\n",
        "            transform_list_len = len(trs)\n",
        "            \n",
        "            for idx in range(transform_list_len):\n",
        "              img = trs[idx](img)     \n",
        "            \n",
        "            heatmap = F.to_tensor(heatmap)\n",
        "            heatmap[heatmap == 0] = 0.1\n",
        "            inputImage = F.to_tensor(inputImage)           \n",
        "        \n",
        "        return {'input':inputImage,'image': img, 'heatmap': heatmap}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtKkQCYQro18"
      },
      "source": [
        "Custom Dataset for  Segmentation Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqAgHHG5CLlr"
      },
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, folder_path,transform=False):\n",
        "        super(SegmentationDataset, self).__init__()\n",
        "        self.root_dir = folder_path\n",
        "        self.filenames = []\n",
        "        self.target_filenames=[]\n",
        "        global resize_mode\n",
        "        global resize_factor\n",
        "        global mode \n",
        "        global trs\n",
        "\n",
        "        file_path = os.path.join(self.root_dir,\"image\")\n",
        "\n",
        "        for file in os.listdir(file_path):\n",
        "            if file.endswith(\".jpg\"):\n",
        "              extension = \".jpg\"\n",
        "            elif file.endswith(\".jpeg\"):\n",
        "              extension = \".jpeg\"\n",
        "            elif file.endswith(\".png\"):\n",
        "              extension = \".png\"\n",
        "            else:\n",
        "              continue\n",
        "\n",
        "            targetfile = file.replace(extension,\".png\")\n",
        "            target_filepath = os.path.join(self.root_dir,\"target\",targetfile)\n",
        "\n",
        "            if os.path.isfile(target_filepath):\n",
        "              self.target_filenames.append(targetfile)\n",
        "              self.filenames.append(file)\n",
        "\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "    \n",
        "            img_path = os.path.join(self.root_dir,\"image\",self.filenames[index])\n",
        "            mask_path = os.path.join(self.root_dir,\"target\",self.target_filenames[index])\n",
        "          \n",
        "            img = Image.open(img_path)\n",
        "            label = cv2.imread(mask_path,0)\n",
        "            inputImage = img.copy()\n",
        "\n",
        "            if(resize_mode == \"resize\"):\n",
        "              img = img.resize((int(64*resize_factor),int(32*resize_factor)),Image.NEAREST)\n",
        "              label = Image.fromarray(np.uint8(label))\n",
        "              label = label.resize((int(16*resize_factor),int(8*resize_factor)),Image.NEAREST)\n",
        "\n",
        "            if trs:\n",
        "              length_transform = len(trs)\n",
        "              \n",
        "              for idx in range(length_transform):\n",
        "                img = trs[idx](img)\n",
        "                                            \n",
        "            inputImage = F.to_tensor(inputImage)\n",
        "            label = F.to_tensor(label)\n",
        "            target = torch.zeros_like(label)\n",
        "            target[(label.min() == label)] = 0\n",
        "            target[((label.min() < label) & (label.max() > label))] = 1\n",
        "            target[(label.max() == label)] = 2\n",
        "            target = target.squeeze()\n",
        "            dataset = {'input':inputImage ,'image': img, 'target': target}\n",
        "            return dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-PIY3yseU-"
      },
      "source": [
        " Implmentatation of Niloofar Azizi*, Hafez Farazi*, and Sven Behnke:\n",
        "Location Dependency in Video Prediction   https://github.com/AIS-Bonn/LocDepVideoPrediction/blob/master/Conv_PGP_LB.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz0kMGN4pcNv"
      },
      "source": [
        "class LocationAwareConv2d(torch.nn.Conv2d):\n",
        "    def __init__(self,lb,gradient,w,h,in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.locationBias=lb\n",
        "        self.locationEncode=torch.autograd.Variable(torch.ones(w,h,3))\n",
        "        if gradient:\n",
        "            for i in range(w):\n",
        "                self.locationEncode[i,:,1]=self.locationEncode[:,i,0]=(i/float(w-1))\n",
        "    def forward(self,inputs):\n",
        "        if self.locationBias.device != inputs.device:\n",
        "            self.locationBias=self.locationBias.to(inputs.get_device())\n",
        "        if self.locationEncode.device != inputs.device:\n",
        "            self.locationEncode=self.locationEncode.to(inputs.get_device())\n",
        "        b=self.locationBias*self.locationEncode\n",
        "\n",
        "        return super().forward(inputs)+b[0:inputs.shape[2],0:inputs.shape[3],0]+b[0:inputs.shape[2],0:inputs.shape[3],1]+b[0:inputs.shape[2],0:inputs.shape[3],2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyL67K_OsnMw"
      },
      "source": [
        "The Decoder block in NimbroNet2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdUIipflpfDr"
      },
      "source": [
        "def decoder_block(block_number, input_channels, output_channels=0,kernel=0,stride=0, padding=0, output_padding=0):\n",
        "  if block_number == 1:\n",
        "    block = nn.Sequential(\n",
        "      nn.ReLU(),\n",
        "      nn.ConvTranspose2d(input_channels,output_channels,kernel,stride, padding,output_padding)\n",
        "      )\n",
        "  elif block_number == 4:\n",
        "    block = nn.Sequential(\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(input_channels),\n",
        "      )\n",
        "  else:\n",
        "    block = nn.Sequential(\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(input_channels),\n",
        "      nn.ConvTranspose2d(input_channels,output_channels,kernel,stride, padding,output_padding)\n",
        "      )\n",
        "  return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxwe4uSRsuuL"
      },
      "source": [
        "NimbroNet2 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isWTS6DIph35"
      },
      "source": [
        "PAD = 0\n",
        "class NimbroNet2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NimbroNet2,self).__init__()\n",
        "    # Encoder \n",
        "    resnet_encoder = models.resnet18(pretrained=True)\n",
        "    del resnet_encoder.avgpool\n",
        "    del resnet_encoder.fc\n",
        "    # Model Encoders (Resnet)\n",
        "    self.resnet_encoder_block1 = nn.Sequential(*list(resnet_encoder.children())[0:5])\n",
        "    self.resnet_encoder_block2 = nn.Sequential(*list(resnet_encoder.children())[5:6])\n",
        "    self.resnet_encoder_block3 = nn.Sequential(*list(resnet_encoder.children())[6:7])\n",
        "    self.resnet_encoder_block4 = nn.Sequential(*list(resnet_encoder.children())[7:8]) # 512\n",
        "\n",
        "    # Model Decoders\n",
        "    self.decoder_block_1 = decoder_block(1, 512, 256, 2, 2, 0, 0)\n",
        "    self.decoder_block_2 = decoder_block(2, 512, 256, 2, 2, 0, 0)\n",
        "    self.decoder_block_3 = decoder_block(3, 512, 128, 2, 2, 0, 0)\n",
        "    self.decoder_block_4 = decoder_block(4, 256)\n",
        "\n",
        "    #intermediate convolutions\n",
        "    self.conv_en3_de1 = nn.Conv2d(256, 256, kernel_size=1)   \n",
        "    self.conv_en2_de2 = nn.Conv2d(128, 256, kernel_size=1) \n",
        "    self.conv_en1_de3 = nn.Conv2d(64, 128, kernel_size=1) \n",
        "    self.locationBias = torch.nn.Parameter(torch.zeros(120,160,3))\n",
        "    \n",
        "    #location dependant bias\n",
        "    self.lbS = LocationAwareConv2d(self.locationBias,True,120+(PAD*2),160+(PAD*2),in_channels=256, out_channels=3, kernel_size=1, stride=1, padding=(1-2)//2+1, bias= True)\n",
        "    self.lbD = LocationAwareConv2d(self.locationBias,True,120+(PAD*2),160+(PAD*2),in_channels=256, out_channels=3, kernel_size=1, stride=1, padding=(1-2)//2+1, bias= True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.resnet_encoder_block1(x)\n",
        "    intermediate_output_1 = self.conv_en1_de3(x)\n",
        "    x = self.resnet_encoder_block2(x)\n",
        "    intermediate_output_2 = self.conv_en2_de2(x)\n",
        "    x = self.resnet_encoder_block3(x)\n",
        "    intermediate_output_3 = self.conv_en3_de1(x)\n",
        "    x = self.resnet_encoder_block4(x)\n",
        "    x = self.decoder_block_1(x)\n",
        "    decoder_block_2_input = torch.cat((x,intermediate_output_3),1)\n",
        "    decoder_block_2_output = self.decoder_block_2(decoder_block_2_input)\n",
        "    decoder_block_3_input = torch.cat((decoder_block_2_output,intermediate_output_2),1)\n",
        "    decoder_block_3_output = self.decoder_block_3(decoder_block_3_input)\n",
        "    decoder_block_4_input = torch.cat((decoder_block_3_output,intermediate_output_1),1)\n",
        "    decoder_block_4_output_0 = self.decoder_block_4(decoder_block_4_input)\n",
        "    decoder_block_4_output_1 = self.lbS(decoder_block_4_output_0)\n",
        "    decoder_block_4_output_2 = self.lbD(decoder_block_4_output_0)\n",
        "    return decoder_block_4_output_1,decoder_block_4_output_2\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnjSgXS4syNL"
      },
      "source": [
        "Defining the criterion for each head and the optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTvOeCRkppwR"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = NimbroNet2()\n",
        "model.to(device)\n",
        "\n",
        "# Criterion for Detection\n",
        "criterion_d = nn.MSELoss()\n",
        "criterion_d.to(device)\n",
        "\n",
        "# Criterion for Segmentation\n",
        "softmaxing = nn.LogSoftmax(dim=1)\n",
        "criterion_s = nn.NLLLoss()\n",
        "criterion_s.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.AdamW(\n",
        "                        [{\"params\":model.resnet_encoder_block1.parameters(), \"lr\": 0.0000001,'weight_decay':10e-12},\n",
        "                        {\"params\":model.resnet_encoder_block2.parameters(), \"lr\": 0.0000001,'weight_decay':10e-12},\n",
        "                        {\"params\":model.resnet_encoder_block3.parameters(), \"lr\": 0.0000001,'weight_decay':10e-12},\n",
        "                        {\"params\":model.resnet_encoder_block4.parameters(), \"lr\": 0.0000001,'weight_decay':10e-12},\n",
        "                        {\"params\":model.decoder_block_1.parameters(), \"lr\": 0.0001,'weight_decay':0},\n",
        "                        {\"params\":model.decoder_block_2.parameters(), \"lr\": 0.0001,'weight_decay':0},\n",
        "                        {\"params\":model.decoder_block_3.parameters(), \"lr\": 0.0001,'weight_decay':0},\n",
        "                        {\"params\":model.decoder_block_4.parameters(), \"lr\": 0.0001,'weight_decay':0},\n",
        "                        {\"params\":model.conv_en3_de1.parameters(), \"lr\": 0.0001,'weight_decay':0},\n",
        "                        {\"params\":model.conv_en2_de2.parameters(), \"lr\": 0.0001,'weight_decay':0},\n",
        "                        {\"params\":model.conv_en1_de3.parameters(), \"lr\": 0.0001,'weight_decay':0},\n",
        "                        {\"params\":model.locationBias, \"lr\": 0.0001,'weight_decay':0}], lr=0.0001,weight_decay=0)\n",
        "\n",
        "# optimizer scheduler for Cyclic learning rate\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.0006,step_size_up = 20000,cycle_momentum=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOc7Xmdls7hR"
      },
      "source": [
        "Detection Datasets & Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJnbbQLUN4nk"
      },
      "source": [
        "                                 ##### Detection Datasets & Dataloaders #####\n",
        "\n",
        "# Augmentation for train and Validation\n",
        "transforms_train = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "                    transforms.ToTensor()]\n",
        "\n",
        "# Augmentation for Testing\n",
        "transforms_test =[transforms.ToTensor()]\n",
        "                \n",
        "##### Detection Dataset ####\n",
        "\n",
        "# batchsize for detection\n",
        "batch_size_detection = 30\n",
        "\n",
        "# The whole dataset for detection\n",
        "dataset_detection = DetectionDataset('/content/drive/My Drive/detection/dataset',transforms_train)\n",
        "\n",
        "\n",
        "### spliting the dataset into 80% for training and 10% validation and 10% testing ###\n",
        "\n",
        "# counting the needed lengths for detection datasets\n",
        "len_dataset_detection = len(dataset_detection)\n",
        "len_train_dataset_detection,len_rest_dataset_detection = int(len_dataset_detection * 0.8),int(len_dataset_detection * 0.2)\n",
        "# add the difference that lost in the division process to the rest of the dataset\n",
        "len_rest_dataset_detection += len_dataset_detection - (len_train_dataset_detection+len_rest_dataset_detection)\n",
        "len_valid_dataset_detection , len_test_dataset_detection = int(len_rest_dataset_detection*0.5),int(len_rest_dataset_detection*0.5)\n",
        "# add the difference that lost in the division process to the validationdataset\n",
        "len_valid_dataset_detection += len_rest_dataset_detection - (len_test_dataset_detection+len_valid_dataset_detection)\n",
        "\n",
        "### splitting datasets ###\n",
        "train_dataset_detection,evaluation_dataset_detection = torch.utils.data.random_split(dataset_detection,[len_train_dataset_detection,len_rest_dataset_detection])\n",
        "validation_dataset_detection,test_dataset_detection = torch.utils.data.random_split(evaluation_dataset_detection,[len_valid_dataset_detection,len_test_dataset_detection])\n",
        "\n",
        "### dataloaders for detection ###\n",
        "train_loader_detection = torch.utils.data.DataLoader(dataset=train_dataset_detection, \n",
        "                                           batch_size=batch_size_detection, \n",
        "                                           shuffle=True)#,**kwargs)\n",
        "val_loader_detection = torch.utils.data.DataLoader(dataset=validation_dataset_detection, \n",
        "                                           batch_size=batch_size_detection, \n",
        "                                           shuffle=True)#,**kwargs)\n",
        "test_loader_detection = torch.utils.data.DataLoader(dataset=test_dataset_detection, \n",
        "                                           batch_size=batch_size_detection, \n",
        "                                           shuffle=True)#,**kwargs)\n",
        "\n",
        "print(\"for Detection: \", len_train_dataset_detection, \" samples for training \", len_valid_dataset_detection, \" samples for validation \", len_test_dataset_detection, \" samlpes for testing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-qMUGgJtBiv"
      },
      "source": [
        "Segmentation Datasets & Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACilcg4a0rGF"
      },
      "source": [
        "                                ##### Segmentation Datasets & Dataloaders #####\n",
        "\n",
        "# Augmentation for segmentation training and validation\n",
        "transforms_train_segmentation = [\n",
        "                 transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "                 transforms.ToTensor()\n",
        "                 ]\n",
        "\n",
        "#### segmentation dataset #####\n",
        "\n",
        "# batch size for segmentation\n",
        "batch_size_segmentation = 30\n",
        "\n",
        "# The whole dataset for segmentation\n",
        "dataset_segmentation = SegmentationDataset('/content/drive/My Drive/dataset',transforms_train_segmentation)\n",
        "\n",
        "\n",
        "### spliting the dataset into 80% for training and 10% validation and 10% testing ###\n",
        "\n",
        "# counting the needed lengths for segmentation datasets\n",
        "len_dataset_segmentation = len(dataset_segmentation)\n",
        "len_train_dataset_segmentation,len_rest_dataset_segmentation = int(len_dataset_segmentation * 0.8),int(len_dataset_segmentation * 0.2)\n",
        "len_rest_dataset_segmentation += len_dataset_segmentation - (len_train_dataset_segmentation+len_rest_dataset_segmentation)\n",
        "len_valid_dataset_segmentation,len_test_dataset_segmentation = int(len_rest_dataset_segmentation*0.5),int(len_rest_dataset_segmentation*0.5)\n",
        "len_valid_dataset_segmentation += len_rest_dataset_segmentation - (len_test_dataset_segmentation+len_valid_dataset_segmentation)\n",
        "\n",
        "# splitting datasets\n",
        "train_dataset_segmentation,evaluation_dataset_segmentation = torch.utils.data.random_split(dataset_segmentation,[len_train_dataset_segmentation,len_rest_dataset_segmentation])\n",
        "validation_dataset_segmentation,test_dataset_segmentation = torch.utils.data.random_split(evaluation_dataset_segmentation,[len_valid_dataset_segmentation,len_test_dataset_segmentation])\n",
        "\n",
        "### dataloaders ###\n",
        "train_loader_segmentation = torch.utils.data.DataLoader(dataset=train_dataset_segmentation, \n",
        "                                           batch_size=batch_size_segmentation, \n",
        "                                           shuffle=True)#,**kwargs)\n",
        "val_loader_segmentation = torch.utils.data.DataLoader(dataset=validation_dataset_segmentation, \n",
        "                                           batch_size=batch_size_segmentation, \n",
        "                                           shuffle=True)#,**kwargs)\n",
        "test_loader_segmentation = torch.utils.data.DataLoader(dataset=test_dataset_segmentation, \n",
        "                                           batch_size=batch_size_segmentation, \n",
        "                                           shuffle=True)#,**kwargs)\n",
        "\n",
        "print(\"for Segmentation: \", len_train_dataset_segmentation, \" samples for training \", len_valid_dataset_segmentation, \" samples for validation \", len_test_dataset_segmentation, \" samlpes for testing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQK1OAiytG42"
      },
      "source": [
        "total variational loss computaion function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BnqNJ9kqDth"
      },
      "source": [
        "def tv_loss(img,loss_type):\n",
        "\n",
        "    tv_loss_weight = 2e-5 # is the total variation loss weight\n",
        "    loss= torch.zeros_like(loss_type)\n",
        "    #loss.requires_grad(False)\n",
        "    for i in range(img.shape[0]):\n",
        "      # Computing the loss for the first channel (channel for background: class 0)\n",
        "      w_variance0 = torch.sum((img[i,0,:,1:] - img[i,0,:,:-1])**2)\n",
        "      h_variance0 = torch.sum((img[i,0,1:,:] - img[i,0,:-1,:])**2)\n",
        "      loss += tv_loss_weight * (w_variance0 + h_variance0)\n",
        "\n",
        "      # Computing the loss for the second channel (channel for field: class 1)\n",
        "      w_variance1 = torch.sum((img[i,1,:,1:] - img[i,1,:,:-1])**2)\n",
        "      h_variance1 = torch.sum((img[i,1,1:,:] - img[i,1,:-1,:])**2)\n",
        "      loss += tv_loss_weight * (w_variance1 + h_variance1)\n",
        "    \n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZeqH1PCtxNu"
      },
      "source": [
        "Train model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BohmCqDMpvSq"
      },
      "source": [
        "trans = transforms.ToPILImage()\n",
        "def train_model(model,train_loader, criterion, optimizer, seg,loss_vector,log_interval):\n",
        "    images = None\n",
        "    heatmaps = None\n",
        "    for j,train_data in enumerate(train_loader):\n",
        "        \n",
        "        images = train_data['image'].to(device)\n",
        "        \n",
        "        # model is in train mode\n",
        "        model.train()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output\n",
        "        outputsS,outputsD = model(images)\n",
        "\n",
        "\n",
        "        ### Calculate Loss ###\n",
        "        if seg:\n",
        "          target = train_data['target'].to(device)\n",
        "          #print(target.shape)\n",
        "          target = target.long()\n",
        "          loss = criterion(softmaxing(outputsS), target)\n",
        "          loss += tv_loss(outputsS,loss)\n",
        "  \n",
        "        else:\n",
        "          heatmaps = train_data['heatmap'].to(device)\n",
        "          outputsD = outputsD.float()\n",
        "          loss = criterion(outputsD, heatmaps)\n",
        "          #train_loss += loss\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # chaning the learning rate of the cyclic learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        if j % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, j * len(images), len(train_loader.dataset),\n",
        "                100. * float(j) / len(train_loader), loss.item()))\n",
        "            loss_vector.append(loss)\n",
        "\n",
        "    # showing the output after training for one epoch        \n",
        "    outputsS,outputsD = model(images)\n",
        "    \n",
        "    if seg == False:\n",
        "      outputsD[outputsD<0] = 0\n",
        "      outputsD[outputsD>1] = 1\n",
        "      plt.imshow(trans(outputsD[0].cpu()))\n",
        "      plt.show()\n",
        "      plt.imshow(trans(heatmaps[0].cpu()))\n",
        "      plt.show()\n",
        "\n",
        "    if seg:\n",
        "      img = outputsS.data.max(1)[1]        \n",
        "      img[img == 0] = 0\n",
        "      img[img == 2] = 255 \n",
        "      img[img == 1] = 128\n",
        "      \n",
        "      plt.imshow(img[0].cpu(), cmap='gray', vmin=0, vmax=255,interpolation='nearest')\n",
        "      plt.show()\n",
        "\n",
        "    plt.imshow(transforms.ToPILImage()(images[0].cpu()))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"lr = \",optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
        "    return loss_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyVGMIc9uEJv"
      },
      "source": [
        "Freezing function : to freeze/unfreeze the given parameters in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K8Txbkm96qo"
      },
      "source": [
        "def freezing(parameters,value):\n",
        "  for param in parameters:\n",
        "    param.requires_grad = value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpJEJtcuB86q"
      },
      "source": [
        "# 2 arrays to monitor the loss of the detection and segmentation\n",
        "loss_d = []\n",
        "loss_s = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sygYs9GuWnF"
      },
      "source": [
        "The first 50 epochs of training: \n",
        "the samples are downsampled, and then upsampled gradually. The parameters of the encoeder are frozen. Checkpoints are saved after every 25 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AtrMGDnqM4u"
      },
      "source": [
        "# for first 50 epochs freez the Resnet Encoder\n",
        "freezing(model.resnet_encoder_block1.parameters(),False)\n",
        "freezing(model.resnet_encoder_block2.parameters(),False)\n",
        "freezing(model.resnet_encoder_block3.parameters(),False)\n",
        "freezing(model.resnet_encoder_block4.parameters(),False)\n",
        "\n",
        "# model is in train mode\n",
        "model.train()\n",
        "\n",
        "### for first 50 epochs resizing the input and target images! ###\n",
        "resize_mode=\"resize\"\n",
        "\n",
        "# number of epochs for resizing\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(\"epoch\",epoch)\n",
        "\n",
        "    # resizing factor to upsample the samples gradually\n",
        "    resize_factor = int((epoch/8)+1)\n",
        "\n",
        "    loss_s = train_model(model,train_loader_segmentation, criterion_s, optimizer,True,loss_s,10)\n",
        "\n",
        "    loss_d = train_model(model,train_loader_detection, criterion_d, optimizer,False,loss_d,20)\n",
        "    \n",
        "    if epoch % 25 == 0:\n",
        "      path = \"/content/drive/My Drive/Final Model Checkpoints/final_whole_mode_withgreatergoalpost_Checkpoint_epoch\" + str(epoch) + \".th\"\n",
        "      torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss_d':loss_d,\n",
        "            'loss_s':loss_s,\n",
        "            'DetectionTrainDataLoader': train_loader_detection,\n",
        "            'SegmentationTrainDataLoader': train_loader_segmentation,\n",
        "            'DetectionTestDataLoader': test_loader_detection,\n",
        "            'DetectionValidationDataLoader': val_loader_detection,\n",
        "            'SegmentationTestDataLoader': test_loader_segmentation,\n",
        "            'SegmentationValidationDataLoader': val_loader_segmentation,\n",
        "            'resize_factor':resize_factor,\n",
        "            }, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml33Qmv1uw98"
      },
      "source": [
        "The next 50 epochs in the training: Fully-sized images are trained and the paramerter of the encoder are unfrozen (the model is trained jointly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIWyK6koig3z"
      },
      "source": [
        "# for Next 50 epochs unfreez the Resnet Encoder\n",
        "freezing(model.resnet_encoder_block1.parameters(),True)\n",
        "freezing(model.resnet_encoder_block2.parameters(),True)\n",
        "freezing(model.resnet_encoder_block3.parameters(),True)\n",
        "freezing(model.resnet_encoder_block4.parameters(),True)\n",
        "\n",
        "# for only first epoch we resize the input and target images because it some time take a large capacity from the GPU!\n",
        "# then we continue with images full-sizes\n",
        "\n",
        "# model is in train mode\n",
        "model.train()\n",
        "\n",
        "resize_mode=\"resize\"\n",
        "resize_factor = 4\n",
        "\n",
        "epochs = 150\n",
        "epoch_init = 50\n",
        "\n",
        "#to be returned from the model training\n",
        "for epoch in range(epoch_init+1, epochs+1):\n",
        "    print(\"epoch\",epoch)\n",
        "\n",
        "    if epoch > (epoch_init+1):\n",
        "      resize_mode = \"\"\n",
        "      resize_factor = 1\n",
        "\n",
        "    loss_d = train_model(model,train_loader_detection, criterion_d, optimizer,False,loss_d,20)\n",
        "    loss_s = train_model(model,train_loader_segmentation, criterion_s, optimizer,True,loss_s,10)\n",
        "    \n",
        "    if epoch % 25 == 0:\n",
        "      path = \"/content/drive/My Drive/Final Model Checkpoints/final_whole_mode_withsmallergoalpost_Checkpoint_epoch\" + str(epoch) + \".th\"\n",
        "      torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss_d':loss_d,\n",
        "            'loss_s':loss_s,\n",
        "            'DetectionTrainDataLoader': train_loader_detection,\n",
        "            'SegmentationTrainDataLoader': train_loader_segmentation,\n",
        "            'DetectionTestDataLoader': test_loader_detection,\n",
        "            'DetectionValidationDataLoader': val_loader_detection,\n",
        "            'SegmentationTestDataLoader': test_loader_segmentation,\n",
        "            'SegmentationValidationDataLoader': val_loader_segmentation,\n",
        "            'resize_factor':1,\n",
        "            }, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmKu115b9pmc"
      },
      "source": [
        "path = \"/content/drive/My Drive/Final Model Checkpoints/Checkpoint_epoch125.th\"\n",
        "checkpoint = torch.load(path)#, map_location=lambda storage, loc: storage)\n",
        "#print(checkpoint['model_state_dict'])\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch_init = checkpoint['epoch']\n",
        "#train_loader_alltogether = checkpoint['train_loader_alltogether']\n",
        "test_loader_detection = checkpoint['DetectionTestDataLoader']\n",
        "val_loader_detection = checkpoint['DetectionValidationDataLoader']\n",
        "test_loader_segmentation = checkpoint['SegmentationTestDataLoader']\n",
        "val_loader_segmentation = checkpoint['SegmentationValidationDataLoader']\n",
        "train_loader_detection = checkpoint['DetectionTrainDataLoader']\n",
        "train_loader_segmentation = checkpoint['SegmentationTrainDataLoader']\n",
        "loss_d = checkpoint['loss_d']\n",
        "loss_s = checkpoint['loss_s']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKFTUsFsxWPS"
      },
      "source": [
        "Ploting the learning rate curve for detection and segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l3CnSXmUqDH"
      },
      "source": [
        "# learning rate for detection\n",
        "plt.plot(range(0,len(loss_d)), loss_d)\n",
        "plt.title(\"learning rate for detection\")\n",
        "plt.ylabel('Train loss')\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.show()\n",
        "\n",
        "# learning rate for segmentation\n",
        "plt.plot(range(0,len(loss_s)), loss_s)\n",
        "plt.title(\"learning rate for segmentation\")\n",
        "plt.ylabel('Train loss')\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOyOhnXuREnh"
      },
      "source": [
        "# transformation for testing\n",
        "trs = [transforms.ToTensor()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FED90gRxfcu"
      },
      "source": [
        "Computing the testing accuracy metrics for segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P306wzPRgoIx"
      },
      "source": [
        "resize_mode= \"\"\n",
        "model.eval()\n",
        "k=1\n",
        "SMOOTH = 1e-6\n",
        "lossv, accv = [], []\n",
        "total_train = 0\n",
        "correct_train = 0\n",
        "acc0 = []\n",
        "acc1 = []\n",
        "acc2 = []\n",
        "iou0 = []\n",
        "iou1 = []\n",
        "iou2 = []\n",
        "for j,train_data in enumerate(test_loader_segmentation):\n",
        "      k +=1\n",
        "\n",
        "      images = train_data['image'].to(device)\n",
        "      images2 = train_data['input'].to(device)\n",
        "      labels = train_data['target'].to(device)\n",
        "      images.requires_grad_(False)\n",
        "      images2.requires_grad_(False)\n",
        "      labels.requires_grad_(False)\n",
        "      \n",
        "      output,_ = model(images)\n",
        "\n",
        "      for i in range(output.shape[0]):\n",
        "        a = output[i].argmax(0)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        b = predicted.cpu().detach().numpy()\n",
        "        a = labels.cpu().detach().numpy() \n",
        "        pred_0 = np.count_nonzero(b == 0)\n",
        "        label_0 = np.count_nonzero(a == 0)\n",
        "        intersection_0 = sum((a==0)[b==0])\n",
        "        acc0.append(intersection_0 / label_0)\n",
        "        union_0 = pred_0 + label_0 - intersection_0\n",
        "        iou = (intersection_0 + SMOOTH) / (union_0 + SMOOTH)\n",
        "        iou0.append(iou)\n",
        "\n",
        "        b = predicted.cpu().detach().numpy()\n",
        "        a = labels.cpu().detach().numpy() \n",
        "        pred_1 = np.count_nonzero(b == 1)\n",
        "        label_1 = np.count_nonzero(a == 1)\n",
        "        intersection_1 = sum((a==1)[b==1])\n",
        "        acc1.append(intersection_1 / label_1)\n",
        "        union_1 = pred_1 + label_1 - intersection_1\n",
        "        iou = (intersection_1 + SMOOTH) / (union_1 + SMOOTH)\n",
        "        iou1.append(iou)\n",
        "\n",
        "        b = predicted.cpu().detach().numpy()\n",
        "        a = labels.cpu().detach().numpy() \n",
        "        pred_2 = np.count_nonzero(b == 2)\n",
        "        label_2 = np.count_nonzero(a == 2)\n",
        "        intersection_2 = sum((a==2)[b==2])\n",
        "        acc2.append(intersection_2 / label_2)\n",
        "        union_2 = pred_2 + label_2 - intersection_2\n",
        "        iou = (intersection_2 + SMOOTH) / (union_2 + SMOOTH)\n",
        "        iou2.append(iou)\n",
        "      \n",
        "      output = output.data.max(1)[1]\n",
        "\n",
        "      for i in range(output.shape[0]):\n",
        "\n",
        "        plt.title('Test image' + str(k))\n",
        "        image = np.uint8(np.asanyarray(transforms.ToPILImage()(images2[i].cpu())))\n",
        "\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "\n",
        "        img = output[i].cpu()\n",
        "        img[img == 0] = 0\n",
        "        img[img == 2] = 255 \n",
        "        img[img == 1] = 128\n",
        "        \n",
        "        plt.imshow(img, cmap='gray', vmin=0, vmax=255,interpolation='nearest')\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G36VqmNtx19B"
      },
      "source": [
        "Calculating the accuracy for each segmented class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noAUBdsSg7nn"
      },
      "source": [
        "resize_mode=\"\"\n",
        "#Calculating accuracy for each segmented class\n",
        "acc00=0\n",
        "acc11=0 \n",
        "acc22 =0\n",
        "for i in acc0:\n",
        "  acc00 += i\n",
        "acc00 /= len(acc0) \n",
        "print('Accuracy for background is' , acc00*100)\n",
        "\n",
        "for i in acc1:\n",
        "  acc11 += i\n",
        "acc11 /= len(acc1) \n",
        "print('Accuracy for field is' , acc11*100)\n",
        "\n",
        "for i in acc2:\n",
        "  acc22 += i\n",
        "acc22 /= len(acc2) \n",
        "print('Accuracy for lines is' , acc22*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNTxJU7x3Xr"
      },
      "source": [
        "Calculating the IOU for each segmented class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEW2XZRthAAl"
      },
      "source": [
        "resize_mode=\"\"\n",
        "# Calculating IOU for each segmented class\n",
        "iou00=0\n",
        "iou11=0 \n",
        "iou22 =0\n",
        "for i in iou0:\n",
        "  #print(i)\n",
        "  iou00 += i\n",
        "iou00 /= len(iou0) \n",
        "print('IOU for background is' , iou00)\n",
        "\n",
        "for i in iou1:\n",
        "  #print(i)\n",
        "  iou11 += i\n",
        "iou11 /= len(iou1) \n",
        "print('IOU for field is' , iou11)\n",
        "\n",
        "for i in iou2:\n",
        "  iou22 += i\n",
        "iou22 /= len(iou2) \n",
        "print('IOU for lines is' , iou22)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMHyesvyMK7"
      },
      "source": [
        "Find the center of the contour to get the centers of detected objects https://www.pyimagesearch.com/2016/02/01/opencv-center-of-contour/#download-the-code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnoUfhhnhPwa"
      },
      "source": [
        "def find_center_contour(image):\n",
        "  \n",
        "  ball_centers = []\n",
        "  goalpost_centers = []\n",
        "  robot_centers = []\n",
        "\n",
        "  # converting image to grayscale image\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "  # blurring this grayscale image\n",
        "  blurred = cv2.GaussianBlur(gray, (3, 3), 2)\n",
        "\n",
        "  # convert the grayscale image to binary image\n",
        "  ret,thresh = cv2.threshold(blurred,250,255,0)\n",
        "\n",
        "  # find contours in the binary image\n",
        "  contours, hierarchy = cv2.findContours(thresh.astype(np.uint8),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  for c in contours:\n",
        "    # calculate moments for each contour\n",
        "    M = cv2.moments(c)\n",
        "\n",
        "    # calculate x,y coordinate of center\n",
        "    if M[\"m00\"] != 0:\n",
        "      cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "    else:\n",
        "      cX = 100000\n",
        "    if M[\"m00\"] != 0:\n",
        "      cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "    else:\n",
        "      cY = 100000\n",
        "    \n",
        "    if (cX != 100000) and (cY != 100000):\n",
        "      # if it has a ball color\n",
        "      if image[cY,cX,0] < 200:     \n",
        "        ball_centers.append((cX,cY))\n",
        "      # if it has goalpost color\n",
        "      elif image[cY,cX,1] < 200:\n",
        "        goalpost_centers.append((cX,cY))\n",
        "      #if it has robot color\n",
        "      elif image[cY,cX,2] < 200:\n",
        "        robot_centers.append((cX,cY))\n",
        "\n",
        "  return ball_centers,goalpost_centers,robot_centers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrEoU_93yWj1"
      },
      "source": [
        "Computing the confusion matrix for the detection accuracy metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym4K2vZQxHDP"
      },
      "source": [
        "def get_confusion_matrix(model_output_centers, ground_truth_centers):\n",
        "  radius = 10 #pixels\n",
        "  TP = 0\n",
        "  FN = 0\n",
        "  FP = 0\n",
        "  \n",
        "  gt_length = len(ground_truth_centers)\n",
        "  model_length = len(model_output_centers)\n",
        "\n",
        "  objects_detected = np.zeros(gt_length)\n",
        "  \n",
        "  for k in range(model_length):\n",
        "    found = -1\n",
        "    for l in range(gt_length):\n",
        "\n",
        "        dist = distance.euclidean(model_output_centers[k],ground_truth_centers[l])\n",
        "        if dist <= radius:\n",
        "          TP +=1\n",
        "          found = l\n",
        "          break\n",
        "    if found == -1: #if found is not changed => object is detected by the model but not in the ground truth \n",
        "      FP += 1 \n",
        "    else:\n",
        "      objects_detected[found] = 1\n",
        "  \n",
        "  # False negative is the number of groundtruth that are not detected by the model\n",
        "  FN = np.count_nonzero(objects_detected == 0)\n",
        "  \n",
        "  return TP, FP, FN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "astjgZzTymP6"
      },
      "source": [
        "Computig the accuracy for detectio metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnHilIUExf2u"
      },
      "source": [
        "resize_mode=\"\"\n",
        "TP_all = torch.zeros(3)\n",
        "FP_all = torch.zeros(3)\n",
        "FN_all = torch.zeros(3)\n",
        "recall = torch.zeros(3)\n",
        "precision = torch.zeros(3)\n",
        "fdr = torch.zeros(3)\n",
        "accuracy = torch.zeros(3)\n",
        "f1 = torch.zeros(3)\n",
        "topil = transforms.ToPILImage()\n",
        "\n",
        "model.eval()\n",
        "for j,test_data in enumerate(test_loader_detection):\n",
        "\n",
        "      images = test_data['image'].to(device)\n",
        "      labels = test_data['heatmap'].to(device)\n",
        "      images.requires_grad_(False)\n",
        "      labels.requires_grad_(False)\n",
        "      \n",
        "      _,output = model(images)\n",
        "      output[output<=0.6] = 0\n",
        "      output[output>1] =1\n",
        "\n",
        "      for i in range(output.shape[0]):\n",
        "\n",
        "        output_ball_centers,output_goalpost_centers,output_robot_centers = find_center_contour(np.asarray(topil(output[i].cpu())))\n",
        "\n",
        "        ground_truth_ball_centers,ground_truth_goalpost_centers,ground_truth_robot_centers = find_center_contour(np.asarray(topil(labels[i].cpu())))\n",
        "        \n",
        "        # get confussion matrix for ball\n",
        "        TP, FP, FN = get_confusion_matrix(output_ball_centers, ground_truth_ball_centers)\n",
        "\n",
        "        TP_all[0] += TP \n",
        "        FP_all[0] += FP \n",
        "        FN_all[0] += FN\n",
        "        \n",
        "        # get confussion matrix for goalpost\n",
        "        TP, FP, FN = get_confusion_matrix(output_goalpost_centers, ground_truth_goalpost_centers)\n",
        "        TP_all[1] += TP \n",
        "        FP_all[1] += FP \n",
        "        FN_all[1] += FN \n",
        "\n",
        "        # get confussion matrix for robot\n",
        "        TP, FP, FN = get_confusion_matrix(output_robot_centers, ground_truth_robot_centers)\n",
        "        TP_all[2] += TP \n",
        "        FP_all[2] += FP \n",
        "        FN_all[2] += FN \n",
        "\n",
        "# Calculating the accuracy metrics\n",
        "for i in range(3):\n",
        "\n",
        "  if (FP_all[i] + TP_all[i] + FN_all[i] ) != 0:\n",
        "   accuracy[i] = TP_all[i] / (FP_all[i] + TP_all[i] + FN_all[i])\n",
        "  \n",
        "  if (TP_all[i] + FN_all[i] ) != 0:\n",
        "    recall[i] = TP_all[i] / (TP_all[i] + FN_all[i] )\n",
        "  \n",
        "  if (TP_all[i] + FP_all[i]) != 0:\n",
        "    precision[i] = TP_all[i] / (TP_all[i] + FP_all[i])\n",
        "    fdr[i] = FP_all[i] / (TP_all[i] + FP_all[i])\n",
        "  \n",
        "  if (precision[i] + recall[i]) != 0:\n",
        "    f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
        "\n",
        "print(\"Accuracy for ball\", accuracy[0])       \n",
        "print(\"Accuracy for goal post\", accuracy[1])\n",
        "print(\"Accuracy for robot\", accuracy[2])\n",
        "\n",
        "print(\"Precision for ball\", precision[0])     \n",
        "print(\"Precision for goal post\", precision[1])\n",
        "print(\"Precision for robot\", precision[2])\n",
        "\n",
        "print(\"Recall for ball\", recall[0])    \n",
        "print(\"Recall for goal post\", recall[1])   \n",
        "print(\"Recall for robot\", recall[2])   \n",
        "\n",
        "print(\"FDR for ball\", fdr[0])         \n",
        "print(\"FDR for goal post\", fdr[1])   \n",
        "print(\"FDR for robot\", fdr[2])   \n",
        "\n",
        "print(\"F1 for ball\", f1[0])      \n",
        "print(\"F1 for goal post\", f1[1])  \n",
        "print(\"F1 for robot\", f1[2])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Q3LiGIy5IH"
      },
      "source": [
        "Computing the validation loss detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-WF3hzhRqjV"
      },
      "source": [
        "# calculating the validation loss for detection\n",
        "resize_mode=\"\"\n",
        "Validation_loss = 0.0\n",
        "\n",
        "trans = transforms.ToPILImage()\n",
        "\n",
        "for j,train_data in enumerate(val_loader_detection):\n",
        "        \n",
        "        images = train_data['image'].to(device)\n",
        "        heatmaps = train_data['heatmap'].to(device)\n",
        "\n",
        "        images.requires_grad_(False)\n",
        "        heatmaps.requires_grad_(False)\n",
        "        \n",
        "        # model is in validation mode\n",
        "        model.eval()\n",
        "        \n",
        "        \n",
        "        # Forward pass to get output\n",
        "        _,outputsD = model(images)\n",
        "    \n",
        "        outputsD[outputsD<=0.6] = 0\n",
        "        outputsD[outputsD>1] = 1\n",
        "\n",
        "        plt.imshow(trans(outputsD[0].cpu()))\n",
        "        plt.show()\n",
        "        plt.imshow(transforms.ToPILImage()(images[0].cpu()))\n",
        "        plt.show()\n",
        "        loss = criterion_d(outputsD, heatmaps)\n",
        "        Validation_loss += loss\n",
        "\n",
        "print(\" Validation loss for detection = \",Validation_loss/len(val_loader_detection))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrq3zVb6y7rM"
      },
      "source": [
        "Computing the validation loss for segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voXRDhh9R649"
      },
      "source": [
        "resize_mode=\"\"\n",
        "# calculating the validation loss for segmentation\n",
        "Validation_loss = 0.0\n",
        "\n",
        "trans = transforms.ToPILImage()\n",
        "\n",
        "for j,train_data in enumerate(val_loader_segmentation):\n",
        "        \n",
        "        images = train_data['image'].to(device)\n",
        "        heatmaps = train_data['heatmap'].to(device)\n",
        "\n",
        "        images.requires_grad_(False)\n",
        "        heatmaps.requires_grad_(False)\n",
        "        \n",
        "        # model is in validation mode\n",
        "        model.eval()\n",
        "        \n",
        "        \n",
        "        # Forward pass to get output\n",
        "        outputsS,_ = model(images)\n",
        "    \n",
        "        img = outputsS.data.max(1)[1]        \n",
        "        img[img == 0] = 0\n",
        "        img[img == 2] = 255 \n",
        "        img[img == 1] = 128\n",
        "        \n",
        "        plt.imshow(img[0].cpu(), cmap='gray', vmin=0, vmax=255,interpolation='nearest')\n",
        "        plt.show()\n",
        "\n",
        "        plt.imshow(transforms.ToPILImage()(images[0].cpu()))\n",
        "        plt.show()\n",
        "\n",
        "        loss = criterion_s(outputsS, heatmaps)\n",
        "        Validation_loss += loss\n",
        "\n",
        "print(\"validation loss for segmentation = \",Validation_loss/len(val_loader_detection))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}